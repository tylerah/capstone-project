{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "073e1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dbd2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in processed data\n",
    "file_path = Path(“./filepathhere.csv”)\n",
    "df_survey = pd.read_csv(file_path\n",
    "df_survey.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c768aab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate data\n",
    "# method 1 \n",
    "df_survey = interpolated df_survey method 1\n",
    "# method 2\n",
    "df_survey = interpolated df_survey method 2\n",
    "# method 3\n",
    "df_survey = interpolated df_survey method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52404a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining features\n",
    "X = df_survey.copy()\n",
    "X = X.drop(“target feature column”, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining target\n",
    "y = df_survey[“target feature column”].ravel()\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6452a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 1 0 1 1 0 1 1 0 0]\n",
      "Data: [[-2.98837186  8.82862715]\n",
      " [ 5.72293008  3.02697174]\n",
      " [-3.05358035  9.12520872]\n",
      " [ 5.461939    3.86996267]\n",
      " [ 4.86733877  3.28031244]\n",
      " [-2.14780202 10.55232269]\n",
      " [ 4.91656964  2.80035293]\n",
      " [ 3.08921541  2.04173266]\n",
      " [-2.90130578  7.55077118]\n",
      " [-3.34841515  8.70507375]]\n"
     ]
    }
   ],
   "source": [
    "# TEMPORARY: create test data\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(centers=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09afa6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58eaa9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f06841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the random forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd8d5fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba7d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "142eff40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted False</th>\n",
       "      <th>Predicted True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual False</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual True</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Predicted False  Predicted True\n",
       "Actual False               10               0\n",
       "Actual True                 0              15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=['Actual False', 'Actual True'], columns=['Predicted False', 'Predicted True']\n",
    "    )\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10e94804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted False</th>\n",
       "      <th>Predicted True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual False</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual True</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Predicted False  Predicted True\n",
       "Actual False               10               0\n",
       "Actual True                 0              15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        25\n",
      "   macro avg       1.00      1.00      1.00        25\n",
      "weighted avg       1.00      1.00      1.00        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display results\n",
    "print('CONFUSION MATRIX')\n",
    "display(cm_df)\n",
    "print('CLASSIFICATION REPORT')\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f0f43b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15240\\850842462.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# store accuracy, recall, and f1 score variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbasic_1_model_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbasic_1_model_accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#basic_1_model_recall = recall\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#basic_1_model_f1 = f1 score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# store accuracy, recall, and f1 score variables\n",
    "basic_1_model_accuracy = accuracy\n",
    "#basic_1_model_recall = recall\n",
    "#basic_1_model_f1 = f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st pause and evaluate\n",
    "recall of “1” value\n",
    "Is the recall score greater than .75?\n",
    "If yes, initial pass succeeds.\n",
    "If no, initial pass fails. \n",
    "\n",
    "If it succeeds….    move on to feature importance and begin identifying and removing values that do not influence the output very much. determine if that improves performance\n",
    "\n",
    "If it fails…\n",
    "If this is your first or second interpolate method pass through, return to top and interpolate through another unused method.\n",
    "If this is your third time, move on to feature importance and begin identifying and removing values that do not influence the output very much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "782b02e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15240\\2582117441.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# calculate feature importance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimportances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mimportances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# calculate feature importance \n",
    "importances = sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop less important columns\n",
    "X = X.drop(“less important columns”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fbb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8362430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the random forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "rf_model = rf_model.fit(X_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7023779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "predictions = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "\tcm, index=[“Actual False”, “Actual True”], columns=[“Predicted False”, “Predicted True”]\n",
    "\t)\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322742bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "print(“CONFUSION MATRIX”)\n",
    "display(cm_df)\n",
    "print(“CLASSIFICATION REPORT”)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store accuracy, recall, and f1 score variables\n",
    "feature_model_accuracy = accuracy\n",
    "feature_model_recall = recall\n",
    "feature_model_f1 = f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store accuracy, recall, and f1 score variables\n",
    "basic_1_model_accuracy = accuracy\n",
    "basic_1_model_recall = recall\n",
    "basic_1_model_f1 = f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "data = [ ,index=[‘Accuracy’, ‘Recall’, ‘F1 Score’]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4db6d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([1, 2, 3])\n",
    "\n",
    "from pathlib import Path  \n",
    "\n",
    "filepath = Path('data/out.csv')  \n",
    "\n",
    "df.to_csv(filepath)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
